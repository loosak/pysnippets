{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loosak/pysnippets/blob/master/01_time_series_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": false,
        "id": "dgb5to4YXhzU",
        "outputId": "043ebd0d-93cd-4c5b-f656-bd47f6c0c070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<html>\n",
              "<link rel=\"stylesheet\" href=\"https://marysia.nl/assets/GDD/css/custom.css\">\n",
              "</html>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Optional: change Jupyter Notebook theme to GDD theme\n",
        "from IPython.core.display import HTML\n",
        "HTML(url='https://gdd.li/jupyter-theme')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSMvbJ90Xhzb"
      },
      "source": [
        "![footer_logo](https://github.com/JTHaywardGDD/time-series/blob/main/images/logo.png?raw=1)\n",
        "# Time Series Analysis\n",
        "\n",
        "- [repo:](https://github.com/JTHaywardGDD/time-series)\n",
        "- [video](https://youtu.be/2b725bplNt8)\n",
        "\n",
        "## Goal\n",
        "\n",
        "Pandas is the core data manipulation and analysis library for Python and it has some amazing utilities for dealing with series time data. \n",
        "\n",
        "The goal of this notebook is to familiarise ourselves with how Pandas can be used to work with Time Series data. \n",
        "\n",
        "We shall use a real Time Series dataset to demonstrate these functionalities and learn some fundamental techniques for Time Series analysis.\n",
        "\n",
        "## Program\n",
        "1. [Time Utilities in Pandas](#timeutil)\n",
        "2. [Reading in Time Series Data](#read)\n",
        "3. [Time-based Manipulations](#mani)\n",
        "4. [Smoothing](#roll)\n",
        "5. [Summary](#sum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fQSZ0WxXXhzn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNon2AQNXhzo"
      },
      "source": [
        "<a id='timeutil'></a>\n",
        "\n",
        "## 1. Time Utilities in Pandas\n",
        "\n",
        "### Timestamps\n",
        "![footer_logo](https://github.com/JTHaywardGDD/time-series/blob/main/images/clock.jpeg?raw=1)\n",
        "\n",
        "In pandas, specific times are represented as **Timestamps**. A Timestamp is the pandas equivalent of python’s Datetime and is interchangeable with it in most cases.\n",
        "\n",
        "Pandas can create datetime data from strings formated as `'yyyy-mm-ddThh:mm:ss:ms'` using `pd.Timestamp()`. The date units are years (‘Y’), months (‘M’), weeks (‘W’), and days (‘D’), while the time units are hours (‘h’) in 24 hour format, minutes (‘m’), seconds (‘s’), milliseconds (‘ms’). Note that time units are combined with date units using `'T'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "id": "XIWqkU7nXhzp",
        "outputId": "9b20e502-4765-482e-a887-955446a8fbde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the date is 2022-03-26 and the time is 09:00:00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Timestamp('2022-03-26 09:00:00')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "date = pd.Timestamp('2022-03-26T09:00:00')\n",
        "print(f'the date is {date.date()} and the time is {date.time()}')\n",
        "date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erYh6tTLXhzq"
      },
      "source": [
        "Pandas Timestamps support a wide range of [operations](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html).\n",
        "\n",
        "For example, we can access various attributes stored in the Timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o7JWmbQKXhzq",
        "outputId": "b174c3a0-b6b3-4cfd-b7d7-ec4232fa0198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "pd.Timestamp('2022-02-20T18:34:56').daysinmonth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XDWIwqzqXhzr",
        "outputId": "76db9959-7a8c-4411-e054-df7891ceafcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "pd.Timestamp('2022-02-20T18:34:56').weekofyear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8D92nWGKXhzu",
        "outputId": "6321115d-0200-408e-cb80-1ebd69d6c0a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "pd.Timestamp('2022-02-20T18:34:56').quarter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P85R3VIXXhzv"
      },
      "source": [
        "We can also perform time based operations and use time related methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s78h1WxNXhzw"
      },
      "outputs": [],
      "source": [
        "pd.Timestamp('2022-02-20T18:34:56') - pd.Timestamp('2020-02-18T18:24:32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9au7IeM0Xhzw"
      },
      "outputs": [],
      "source": [
        "pd.Timestamp('2022-02-20T18:34:56').month_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URxQ-tK7Xhzx"
      },
      "outputs": [],
      "source": [
        "pd.Timestamp('2022-02-20T18:34:56').day_name()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuKihK0qXhzx"
      },
      "source": [
        "<a id='ex'></a>\n",
        "### <mark>Exercise: Investigate the timestamp features and methods\n",
        "\n",
        "We've seen a few examples, but let's investigate further.\n",
        "- What day of the year is it today\n",
        "- Are we in a leap year?\n",
        "- How long is it until a Public Holiday (e.g. Christmas)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xSQBQ8_jXhzy",
        "outputId": "04fb67b1-ea82-4c7f-d68c-57e3138d2dfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Timestamp('2022-05-24 18:41:05.746409')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "current_date = pd.Timestamp('today')\n",
        "current_date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufqr8eyxXhzy"
      },
      "source": [
        "<a id='read'></a>\n",
        "![footer_logo](https://github.com/JTHaywardGDD/time-series/blob/main/images/air-quality.jpeg?raw=1) \n",
        "\n",
        "\n",
        "## 2. Reading in Time Series Data\n",
        "\n",
        "\n",
        "Throughout this taster will use a dataset containing daily air quality index in Californian counties between 2007 and 2017 (based on a larger dataset from [Kaggle](https://www.kaggle.com/epa/carbon-monoxide)). \n",
        "\n",
        "Each datapoint indicates the average air quality index on a certain day: the higher the value - the more polluted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AVxAP6M9Xhzz",
        "outputId": "ec8ec483-13ae-4242-89dd-1fb7f3655c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ad5cf2bb0b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mair_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/air_quality.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mair_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/air_quality.csv'"
          ]
        }
      ],
      "source": [
        "air_df = pd.read_csv('data/air_quality.csv')\n",
        "air_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8N8BZnXhzz"
      },
      "source": [
        "Typically time data is contained in a separate column of standard strings; notice how our time data is not currently recognised as Timestamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ujI3yhkXhzz"
      },
      "outputs": [],
      "source": [
        "air_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DGq7nAjXhzz"
      },
      "source": [
        "In order to make our time data machine readable, we can set `parse_dates` with the list of columns to be converted to Pandas Timestamps when reading the data with `pd.read_csv`. This automatically identifies the format of the dates, although specific formatting is also possible.\n",
        "\n",
        "For most time-series analysis functionality, we also benefit from setting the dates as the index in the Pandas DataFrame. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulo6QZL7Xhz0"
      },
      "outputs": [],
      "source": [
        "air_df = pd.read_csv('data/air_quality.csv', index_col='date_local', parse_dates=True)\n",
        "air_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgCaeohoXhz0"
      },
      "outputs": [],
      "source": [
        "air_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wANBDUEtXhz0"
      },
      "source": [
        "With the Timestamps as the index, we can directly filter on our DataFrame using the`loc` method and easily produce plots to visualise our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tAOj2gxXhz0"
      },
      "outputs": [],
      "source": [
        "air_df.loc['2008-02-01':'2008-02-10']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55E3LlyeXhz1"
      },
      "outputs": [],
      "source": [
        "air_df.plot(figsize=(16,4));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM-FAbrlXhz1"
      },
      "source": [
        "<a id='mani'></a>\n",
        "\n",
        "## 3. Time-based manipulations\n",
        "\n",
        "### Easy Aggregations\n",
        "\n",
        "Another advantage of the datetime-index approach is that it provides us with some functionality for easy time-based aggregations. One such aggregation is `resample`. \n",
        "\n",
        "For example, we can easily calculate the _mean_ per _year_ by running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meu83JQ6Xhz2"
      },
      "outputs": [],
      "source": [
        "air_df.resample('Y').mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZpy1-h-Xhz2"
      },
      "source": [
        "You can also run the same aggregation per month `M`, week `W`, day `D` or quarter `Q`. Custom aggregation periods are also possible, for example per 4 weeks `4W` or per 3 months `3M`. If the index is a timestamp that also includes times, then you can also aggregate per hour. See [here] for a more comprehensive list of offsets, that can be as specific as _'Business Month Begin'_. \n",
        "\n",
        "[here]: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\n",
        "\n",
        "In the cell below we compute the mean AQI for consecutive 4 week periods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "sr_mn5uQXhz2"
      },
      "outputs": [],
      "source": [
        "air_df.resample('4W').mean().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjBktHiTXhz3"
      },
      "source": [
        "We can also use general `.agg()` methods here to apply multiple aggregators, including custom aggregations. For example, the spread per month: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDgb_hK4Xhz3"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    air_df\n",
        "    .resample('M')\n",
        "    .agg([\n",
        "        ('mean','mean'),\n",
        "        ('var','var'),\n",
        "        ('spread', lambda month_df: month_df.max() - month_df.min())\n",
        "    ])\n",
        "    #.droplevel(0, axis=1)\n",
        "    .head()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsCA0Eh6Xhz3"
      },
      "source": [
        "### Time Based Features \n",
        "\n",
        "Any column in Pandas that is of dtype `datetime` has a module attached that can be used to perform vectorised datetime operations. This is very similar to the `.str` module attached to string columns. It is a good thing to explore since the alternative is non-vectorised and much slower.\n",
        "\n",
        "Below is an example of getting the quarter and adding the day name. Feel free to explore [other](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#api-series-dt) properties and methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "oF2mlznXXhz3"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    air_df\n",
        "    .assign(quarter = lambda df: df.index.quarter,\n",
        "            weekday = lambda df: df.index.day_name())\n",
        "    .head()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTUG2Y6hXhz4"
      },
      "source": [
        "### <mark>Exercise: Resampling\n",
        "\n",
        "Compute the average AQI per week and find the week that had the worst (or highest) AQI\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlm5ylueXhz4"
      },
      "outputs": [],
      "source": [
        "# %load answers/resample.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4P5IjnFXhz4"
      },
      "source": [
        "### <mark>Exercise: Weekday with worst air quality</mark>\n",
        "\n",
        "Which day of the week has the worst (or highest) on average? Does AQI drop in the weekend? \n",
        "\n",
        " <font color='green'>Bonus </font>: Make a bar plot to better illustrate any weekly air quality patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD4MibqEXhz5"
      },
      "outputs": [],
      "source": [
        "# %load answers/time_based_features.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OvPdJsVXhz5"
      },
      "source": [
        "### <font color='green'>BONUS SECTION: </font> Shifting\n",
        "\n",
        "It can be often useful to shift some variables forward or backwards in time. This can for example help us create variables with lagged values or calculate differences in values between time steps. This can be done using Panda's `shift` method, which can shift values by a given number of periods (positive or negative). \n",
        "\n",
        "The example below uses this method to create a new variable for AQI during the previous day:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw7RFkZOXhz5"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    air_df\n",
        "    .assign(aqi_yesterday = lambda df: df.aqi.shift(1))\n",
        "    .assign(change_in_aqi = lambda df: df.aqi - df.aqi_yesterday)\n",
        "    .dropna()\n",
        "    .head()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqQR9X2_Xhz5"
      },
      "source": [
        "\n",
        "\n",
        "<a id='roll'></a>\n",
        "\n",
        "## 4. Smoothing\n",
        "\n",
        "Let us have a closer look at the air quality patterns during a single year. \n",
        "\n",
        "The simplest way to plot timestamp data dynamics in Pandas is using the `plot` method, which by default plots a linear plot over time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ruhYfqWXhz5"
      },
      "outputs": [],
      "source": [
        "air_df_2016 = air_df.loc['2016']\n",
        "\n",
        "air_df_2016.plot(figsize=(18,6));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byd1-h3HXhz6"
      },
      "source": [
        "### Rolling Average smoothing\n",
        "\n",
        "Various patterns can be seen on this daily line graph, but the overall trend can be hard to see due to noise plentiful short spikes. \n",
        "\n",
        "In order to see a *smoother* pattern over time, a __rolling average__ can be applied to a Time Series. It walks over the timestamps with a given window (7 days for example) and calculates averages for each. \n",
        "\n",
        "Pandas can perform this via the `rolling` method which can be called on both a DataFrame as well as a Series object. The window size for this method can be set using both a fixed number of data points as well as particular time intervals (days, weeks etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEUqaSH6Xhz6"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    air_df_2016\n",
        "    .assign(rolling_mean=lambda df: df['aqi'].rolling('20D').mean())\n",
        "    .plot(figsize=(16, 4))\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JJH_wA0Xhz6"
      },
      "source": [
        "Note that the orange *rolling mean line* is lagging behind what actually happens. This is because, by default, each point of the rolling average represents information about this day and the preceding days - not just this particular moment! We can remove this effect using *centering*.\n",
        "\n",
        "To center the rolling mean, we can either manually shift it backwards or use the option `center=True` for the `rolling()` method. You can see below how both achieve the same result - the red and green (overlapping) lines do not lag anymore. \n",
        "\n",
        "__Importantly__, centering requires information from the future for each point. This makes centering bad practice if we want to further make predictions about the future — this information is then already contained in the present data points, which is referred to as *information leakage* in machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "codk1zs_Xhz6"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    air_df_2016\n",
        "    .assign(\n",
        "        rolling_mean=lambda df: df['aqi'].rolling('20D').mean(),\n",
        "        # With `center=True` window size cannot be a time frame!\n",
        "        rolling_mean_center=lambda df: df['aqi'].rolling(20, center=True).mean(),\n",
        "        manual_center=lambda df: df['rolling_mean'].shift(-9)\n",
        "    )\n",
        "    .plot(figsize=(16,4), title='rolling_mean_center and manual_center overlap')\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYmq3F5_Xhz6"
      },
      "source": [
        "Rolling average smoothing is a simple way to isolate signal from noise in Time Series data and get an idea about general Time Series behavior.\n",
        "\n",
        "However, there are some notable drawbacks:\n",
        "\n",
        "- Highly dependent on window size: \n",
        "    - using a small window sizes can lead to more noise than signal;\n",
        "    - using a large window size can remove important signal information).\n",
        "- It always lags by the window size (unless centered).\n",
        "- It is not really informative about the future.\n",
        "- It can be significantly skewed by extreme datapoints in the past."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2FPKCi6Xhz7"
      },
      "source": [
        "### <mark>Exercise: Weekday with worst air quality</mark>\n",
        "\n",
        "In a previous exercise we saw that the week commencing Monday 24th December 2007 had the worst average air quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4saPEVzXhz7"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    air_df\n",
        "    .resample('W-Mon')\n",
        "    .mean()\n",
        "    .nlargest(1, 'aqi')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUKjt0QcXhz7"
      },
      "source": [
        "However, this is not necessarilly the 7-day period that had the worst average air quality.\n",
        "\n",
        "For example, that may span a Friday to Thursday, as opposed to Monday to Sunday.\n",
        "\n",
        "Find the 7-day period that had the worst average air quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P-C8Ph0Xhz8"
      },
      "outputs": [],
      "source": [
        "# %load answers/rolling.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-teRWjSwXhz8"
      },
      "source": [
        "### <font color='green'>BONUS SECTION: </font> Exponential Smoothing\n",
        "\n",
        "An alternative to calculating the rolling statistics is to smooth the timeseries exponentially with the following formula:\n",
        "\n",
        "$$\\hat{y_t} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1}$$\n",
        "\n",
        "where $\\hat{y_t}$ is the output of the exponential smoothing at time $t$, $y_t$ is the data point at time $t$, and $0<\\alpha<1$ is the *the smoothing factor*.\n",
        "\n",
        "\n",
        "The idea is to recursively smooth the series by averaging the current average with the current value. If $\\alpha$ is high then the smoothing will be low but the average can respond quicker to changes, and if it is low — the result will be much more smooth and flat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlbQ-PPYXhz8"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    air_df_2016\n",
        "    .assign(\n",
        "        smoothed_01=lambda df: df['aqi'].ewm(alpha=0.1).mean(),\n",
        "        smoothed_001=lambda df: df['aqi'].ewm(alpha=0.01).mean()\n",
        "    )\n",
        "    .plot(figsize=(16, 4))\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHab7UTXXhz9"
      },
      "source": [
        "Exponential Smoothing exhibits reduced lagging and more weight assigned to the current timestamps compared to Simple Rolling Averages, which also makes it more informative about the future. You can read more about EWM (exponentially weighted Moving average) in the pandas [docs].\n",
        "\n",
        "[docs]: https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#exponentially-weighted-windows\n",
        "\n",
        "Other alternatives are [weighted smoothing](http://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#rolling-windows) and [expanding windows](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.expanding.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drjoA040Xhz9"
      },
      "source": [
        "<a id='sum'></a>\n",
        "## 5. Summary\n",
        "\n",
        "We have covered: \n",
        "- Timestamps and formatting in Pandas\n",
        "- How to properly read in Time Series data in Pandas, and why it is important to set the date as an index\n",
        "- Time based manipulations, such as aggregations with `resample`, time-based features based on dtype `datetime` and shifting\n",
        "- Smoothing with rolling averages, its disadvantages and some alternatives.\n",
        "\n",
        "We should now be able to answer analytics questions like: \n",
        "- Which year had the worst air quality?\n",
        "- Which five day period had the largest decrease in air quality (between the first and last day)?\n",
        "- Does air quality improve over the weekend? \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "01_time_series_analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}